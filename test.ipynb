{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b835ca0d-c10d-461b-b7fa-332ca9ad901e",
   "metadata": {},
   "source": [
    "# Ship Detection from Satellite Imagery Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6103ac7c-a38e-4112-b7ba-b9b8ffddda10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 80, 80, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 19200)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "X = np.load('ships_dataset/ship_data.npy')\n",
    "t = np.load('ships_dataset/ship_labels.npy')\n",
    "\n",
    "label_names = ['no_ship', 'ship']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size = 0.2, random_state = 91)\n",
    "print(X_test.shape)\n",
    "X_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "X_flat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc06f881-ea4b-47d9-be9e-fbd02e67d300",
   "metadata": {},
   "source": [
    "## Performance & Inference Metrics on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fbe6def-1481-4b24-957e-d364db955634",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: svmModel.pkl\n",
      "  F1 Score: 0.7677\n",
      "  Total inference time: 3.6666 sec\n",
      "  Avg time per sample: 0.004583 sec\n",
      "\n",
      "Model: svmLLE.pkl\n",
      "  F1 Score: 0.0000\n",
      "  Total inference time: 2.0357 sec\n",
      "  Avg time per sample: 0.002545 sec\n",
      "\n",
      "Model: logregModel.pkl\n",
      "  F1 Score: 0.7990\n",
      "  Total inference time: 0.0566 sec\n",
      "  Avg time per sample: 0.000071 sec\n",
      "\n",
      "Model: svmIso.pkl\n",
      "  F1 Score: 0.8324\n",
      "  Total inference time: 1.7153 sec\n",
      "  Avg time per sample: 0.002144 sec\n",
      "\n",
      "Model: logIso.pkl\n",
      "  F1 Score: 0.8404\n",
      "  Total inference time: 1.6647 sec\n",
      "  Avg time per sample: 0.002081 sec\n",
      "\n",
      "Model: logLLE.pkl\n",
      "  F1 Score: 0.0000\n",
      "  Total inference time: 1.8648 sec\n",
      "  Avg time per sample: 0.002331 sec\n",
      "\n",
      "Model: svmPCA.pkl\n",
      "  F1 Score: 0.8495\n",
      "  Total inference time: 0.1613 sec\n",
      "  Avg time per sample: 0.000202 sec\n",
      "\n",
      "Model: logPCA.pkl\n",
      "  F1 Score: 0.8333\n",
      "  Total inference time: 0.1485 sec\n",
      "  Avg time per sample: 0.000186 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(path, X, y):\n",
    "    model = joblib.load(path)\n",
    "\n",
    "    start = time.time()\n",
    "    preds = model.predict(X)\n",
    "    end = time.time()\n",
    "\n",
    "    total_time = end - start\n",
    "    avg_time = total_time / len(X)\n",
    "\n",
    "    f1 = f1_score(y, preds)\n",
    "\n",
    "    return f1, total_time, avg_time\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    \"logIso.pkl\",\n",
    "    \"logLLE.pkl\",\n",
    "    \"logPCA.pkl\",\n",
    "    \"svmIso.pkl\",\n",
    "    \"svmLLE.pkl\",\n",
    "    \"svmPCA.pkl\",\n",
    "    \"logregModel.pkl\",  \n",
    "    \"svmModel.pkl\",   \n",
    "}\n",
    "\n",
    "for path in model_paths:\n",
    "    f1, total_time, avg_time = evaluate_model(path, X_flat, t_test)\n",
    "    print(f\"Model: {path}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  Total inference time: {total_time:.4f} sec\")\n",
    "    print(f\"  Avg time per sample: {avg_time:.6f} sec\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b6e3d-d359-4294-ab94-3877c87a8a2b",
   "metadata": {},
   "source": [
    "| Model Description                               | F1 Score | Total Inference Time (s) | Avg Time per Sample (s) |\n",
    "| ----------------------------------------------- | -------- | ------------------------ | ----------------------- |\n",
    "| **Support Vector Machine (SVM)**                | 0.7677   | 3.6666                   | 0.004583                |\n",
    "| **SVM with LLE (Locally Linear Embedding)**     | 0.0000   | 2.0357                   | 0.002545                |\n",
    "| **Logistic Regression**                         | 0.7990   | 0.0566                   | 0.000071                |\n",
    "| **SVM with Isomap**                             | 0.8324   | 1.7153                   | 0.002144                |\n",
    "| **Logistic Regression with Isomap**             | 0.8404   | 1.6647                   | 0.002081                |\n",
    "| **Logistic Regression with LLE**                | 0.0000   | 1.8648                   | 0.002331                |\n",
    "| **SVM with PCA (Principal Component Analysis)** | 0.8495   | 0.1613                   | 0.000202                |\n",
    "| **Logistic Regression with PCA**                | 0.8333   | 0.1485                   | 0.000186                |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
